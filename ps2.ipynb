{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving Krussel-Smith with Deep Equilibrium Nets\n",
    "*Simon Lebastard, 02/07/2023*\n",
    "\n",
    "## Krussel-Smith (1998) model\n",
    "In the KS'98 model, we look a flavour of the stochastic growth model with a continuum of households under partially uninsurable risk.\n",
    "\n",
    "**Demand side**\n",
    "$$V(c) = \\mathrm{E}_0\\Big[\\sum_{t=0}^{\\infty}{\\beta^t U(c_t)}\\Big]$$\n",
    "with\n",
    "$$U(c) = \\lim_{\\nu \\to \\sigma}{\\frac{c^{1-\\nu} - 1}{1 - \\nu}}$$\n",
    "\n",
    "Agents are each endowed with $\\epsilon \\tilde{l}$ units of labor per period, where $\\epsilon$ is a first-order Markov chain. (In K&S'98, $\\epsilon$ can only take two values: 0 and 1, representing unemployed and employed idiosyncratic states, respectively).\n",
    "As a very large number of agents is consided, and by assumed independence of the idiosyncratic shocks, K&S'98 assume that the total number of employed and unemployed people remain constant over time. That is, there is no aggregate fluctuations of jobs supply.\n",
    "\n",
    "Idiosyncratic endogenous state: $k$ holding of capital\n",
    "Idiosyncratic exogenous state: $\\epsilon$ at the individual level\n",
    "$\\Gamma$ the joint distribution of idiosyncratic states $(k,\\epsilon)$\n",
    "\n",
    "Given the aggregate states (see $z$ defined below), the consumer's optimization problem is:\n",
    "$$v(k,\\epsilon;\\Gamma,z) = \\max_{c,k'}{ \\Big\\{ U(c) + \\beta\\mathrm{E}\\Big[v(k',\\epsilon';\\Gamma',z') \\mid z,\\epsilon \\Big] \\Big\\} } $$\n",
    "under budget constraint, rational expectations wrt law of motion and non-negative capital holding.\n",
    "\n",
    "The budget constraint writes:\n",
    "$$c+k' = r(\\hat{k},\\hat{l},z)k + w(\\hat{k},\\hat{l},z)\\tilde{l}\\epsilon + (1-\\delta)k$$\n",
    "\n",
    "**Supply side**\n",
    "A single-type good is produced using two factors of production: labor $l$ and capital $k$.\n",
    "The good is produced according to a Cobb-Douglas production function:\n",
    "$$y = zk^{\\alpha}l^{1-\\alpha}, \\quad \\alpha \\in [0,1]$$\n",
    "The TFP is stochastic and coresponds to the source of aggregate risk. In K&S'98, two aggregate states are considered: $(z_b, z_g)$.\n",
    "Again, $z$ follows a first-order Markov chain.\n",
    "\n",
    "We assume the production market to be competitive, such that wages $w$ and rental rates $r$, both functions of aggregate states, are respectively determined by:\n",
    "$$w(\\hat{k},\\hat{l},z) = (1 - \\alpha)z(\\frac{\\hat{k}}{\\hat{l}})^\\alpha$$\n",
    "$$r(\\hat{k},\\hat{l},z) = \\alpha z(\\frac{\\hat{k}}{\\hat{l}})^{\\alpha-1}$$\n",
    "\n",
    "Here we assume that as the population of households is infinitely large, the share of unemployed remains constant. Moreover, here consumers have no disutility from labor, implying that the labor supply at each period is constant at $L_s = N*\\mathrm{E}\\big[\\epsilon\\big]$.\n",
    "We have the market clearing condition for the capital/cons good: $$\\int{(c(k,\\epsilon;\\Gamma,z) + k'(k,\\epsilon;\\Gamma,z))dF(\\Gamma)} = (1-\\delta)\\int{k dF(\\Gamma)} + z\\int{k dF(\\Gamma)}^{\\alpha}L_s^{1-\\alpha}$$\n",
    "\n",
    "For both Markov chains, we assume the economy is already running at the stationary distribution.\n",
    "\n",
    "**Law of motion**\n",
    "$$\\Gamma' = H(\\Gamma,z,z')$$\n",
    "\n",
    "\n",
    "### Formulating the problem for solving with DEN\n",
    "Here we will consider two \"implicit\" policy functions for which we solve:\n",
    "- Next-period capital $k'(k,\\epsilon;\\Gamma,z)$\n",
    "- The Lagrange multiplier on the next-period capital non-negativity constraint: $\\mu_k(k,\\epsilon;\\Gamma,z)$\n",
    "- The Lagrange multiplier on the positivity of current-period consumption, ie of the residual of the budget constraint: $\\mu_c(k,\\epsilon;\\Gamma,z)$\n",
    "\n",
    "As in Azinovic et al, we simulate $N$ agents, and index $i \\in [1,...,N]$.\n",
    "\n",
    "#### Idiosyncratic error terms\n",
    "The Euler equation can be obtained by taking the FOC of the consumer's objective function with respect to next-period capital:\n",
    "$$1 = \\beta(1-\\delta)\\mathrm{E}\\Big[\\Big(\\frac{c(k,\\epsilon;\\Gamma,z)}{c(k',\\epsilon';\\Gamma',z')}\\Big)^{\\nu} \\mid z,\\epsilon\\Big] - \\big(\\mu_c(k,\\epsilon;\\Gamma,z) - \\mu_k(k,\\epsilon;\\Gamma,z) \\big)c(k,\\epsilon;\\Gamma,z)^{\\nu}$$\n",
    "\n",
    "Based on that, we defined the Euler error as:\n",
    "$$e_{EE}(k,\\epsilon,\\Gamma,z) \\equiv \\bigg[\\beta(1-\\delta)\\mathrm{E}\\Big[\\Big(\\frac{c(k,\\epsilon;\\Gamma,z)}{c(k',\\epsilon';\\Gamma',z')}\\Big)^{\\nu} \\mid z,\\epsilon\\Big] - \\big(\\mu_c(k,\\epsilon;\\Gamma,z) - \\mu_k(k,\\epsilon;\\Gamma,z) \\big)\\bigg]^{-\\frac{1}{\\nu}} - 1$$\n",
    "and\n",
    "$$e_{EE,i} \\equiv e_{EE}(k_i,\\epsilon_i,\\Gamma,z)$$\n",
    "\n",
    "We define the error on the complementary-slackness condition on k as:\n",
    "$$e_{CS_k}(k,\\epsilon,\\Gamma,z) \\equiv \\mu_k(k,\\epsilon;\\Gamma,z)k'(k,\\epsilon;\\Gamma,z)$$\n",
    "and\n",
    "$$e_{CS_k,i} \\equiv e_{CS_k}(k_i,\\epsilon_i,\\Gamma,z)$$\n",
    "\n",
    "We define the error on the complementary-slackness condition on c as:\n",
    "$$e_{CS_c}(k,\\epsilon,\\Gamma,z) \\equiv \\mu_c(k,\\epsilon;\\Gamma,z)c(k,\\epsilon;\\Gamma,z)$$\n",
    "and\n",
    "$$e_{CS_c,i} \\equiv e_{CS_c}(k_i,\\epsilon_i,\\Gamma,z)$$\n",
    "\n",
    "#### Aggregate error term\n",
    "At each period, we compute the aggregate $K \\equiv \\sum_{i=1}^{N}{k_i}$\n",
    "\n",
    "One way to proceed could be to define an error based on the market clearing condition on capital. Instead, we will enforce consumption to satisfy the market clearing condition at each period. Note that in doing so, we could still need to enforce that consumption is positive. Instead of enforcing a new constraint on a Lagrangian multiplier associated with consumption, I compute the error on MC by constraining consumption to be positive in the error, by using a transformation of the error that satisfies:\n",
    "$$\\lim_{c \\downarrow 0}{e_{MC}(c)} = \\infty$$\n",
    "This should prevent consumption from ever being non-positive.\n",
    "\n",
    "We define the error on the market clearing condition as:\n",
    "$$e_{MC} \\equiv \\sum_{i=1}^{N}{\\Big[c(k_i,\\epsilon_i;\\Gamma,z) + k'(k_i,\\epsilon_i;\\Gamma,z)\\Big]} - zK^{\\alpha}(Nu)^{1-\\alpha} - (1-\\delta)K$$\n",
    "\n",
    "#### Defining the loss function\n",
    "On a batch $\\mathcal{D}_{train}$, the loss function is defined as:\n",
    "$$l(\\theta) \\equiv \\frac{1}{\\mid\\mathcal{D}_{train}\\mid} \\sum_{x \\in \\mathcal{D}_{train}}{\\frac{1}{N-1}\\sum_{i=1}^{N}{\\Big(e_{EE,i}^2 + e_{CS_k,i}^2 + e_{CS_c,i}^2\\Big)} + e_{MC}^2}$$\n",
    "\n",
    "**Note: alternative implementation**<br>\n",
    "Instead of having a policy variable for the Lagrange multiplier on the positivity of next-period capital, we could have a transformation function on  next-period capital that ensures it remains non-negative at all times. I will implement this alternative method and compare results.\n",
    "\n",
    "#### Defining network dimensions\n",
    "As we make consumption implicit, we have the following dimensions:\n",
    "- $2N+1$ scalar inputs: $(z, \\big\\{(k_i,\\epsilon_i)\\big\\}_{i \\in [1..N]})$\n",
    "- $4N$ scalar outputs: ${k_i', c_i, \\mu_{k,i}, \\mu_{c,i}}_{i \\in [1..N]}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-09 12:13:02.609031: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "# Import modules\n",
    "import os\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.config.run_functions_eagerly(True)\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from keras.models import Model\n",
    "from keras.layers import * #Input, Dense, BatchNormalization\n",
    "from tensorflow import Tensor\n",
    "\n",
    "# Set the seed for replicable results\n",
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "#tf.set_random_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def firm(K,z):\n",
    "    prod = z*tf.pow(K, α)*tf.pow(L, 1-α)\n",
    "    r = z*α*tf.pow(K, α-1)*tf.pow(L, 1-α)\n",
    "    w = z*(1-α)*tf.pow(K, α)*tf.pow(L, -α)\n",
    "    return prod, r, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = num_agents = 100\n",
    "\n",
    "# Neural network architecture parameters\n",
    "num_input_nodes = 2*N + 1  # Dimension of extended state space (8 aggregate quantities and 4 distributions)\n",
    "num_output_nodes = 2*N  # Output dimension (capital holding for each agent. Agent 1 is born without capital (k^1=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-09 12:13:05.765667: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.random import stateless_binomial\n",
    "\n",
    "# Shocks structure\n",
    "Z = tf.constant([0.9, 1.1], dtype=tf.float32)\n",
    "z_l = Z[0]\n",
    "z_h = Z[1]\n",
    "P_z = tf.constant([[0.9, 0.1], [0.1, 0.9]], dtype=tf.float32)\n",
    "def draw_z(z):\n",
    "    z_id = int(z==z_h)\n",
    "    zp_id = stateless_binomial(shape=[1,], counts=1, probs=P_z[z_id,:])\n",
    "    return Z[zp_id]\n",
    "\n",
    "E = tf.constant([0, 1], dtype=tf.float32)\n",
    "eps_l = E[0]\n",
    "eps_h = E[1]\n",
    "P_eps = tf.constant([[0.5, 0.5], [0.5, 0.5]], dtype=tf.float32)\n",
    "def draw_eps(N: int):\n",
    "    return stateless_binomial(\n",
    "        shape=[N,],\n",
    "        seed=[123, 456],\n",
    "        counts=1,\n",
    "        probs=0.5,\n",
    "        output_dtype=tf.float32,\n",
    "    )\n",
    "\n",
    "# Other constants\n",
    "α = tf.constant(0.3, dtype=tf.float32)\n",
    "β = tf.constant(0.7, dtype=tf.float32)\n",
    "δ = tf.constant(0.1, dtype=tf.float32)\n",
    "γ = tf.constant(2.0, dtype=tf.float32)\n",
    "L = N*tf.reduce_mean(E)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network - Architecture\n",
    "\n",
    "The following is a specialization class of Keras' Model, with a custom training step with gradient taping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def budget_residual(k: Tensor, c: Tensor, eps:Tensor, kp: Tensor, r:float, w:float):\n",
    "    return w*eps + (1+r-δ)*k - kp - c\n",
    "\n",
    "def FB(a:float, b:float):\n",
    "    return a + b - tf.sqrt(tf.pow(a,2) + tf.pow(b,2))\n",
    "\n",
    "lr = 0.00001\n",
    "\n",
    "initializer = tf.keras.initializers.GlorotUniform()\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate=lr,\n",
    ")\n",
    "\n",
    "class DENModel(Model):\n",
    "    @tf.function\n",
    "    def forward(self, z:Tensor, k:Tensor, eps:Tensor):\n",
    "        N = k.shape[0]\n",
    "        kp = tf.Variable(tf.zeros_like(k))\n",
    "        mup = tf.Variable(tf.zeros_like(k))\n",
    "        c = tf.Variable(tf.zeros_like(k))\n",
    "        lambdap = tf.Variable(tf.zeros_like(k))\n",
    "\n",
    "        K = tf.math.reduce_sum(k)\n",
    "        x_aggr = tf.concat([z[None],k],axis=0)\n",
    "        x_aggr = tf.reshape(x_aggr, shape=[1,-1])\n",
    "        for agent_id in range(N):\n",
    "            x_idio = tf.concat([k[agent_id][None], eps[agent_id][None]], axis=0)\n",
    "            x_idio = tf.reshape(x_idio, shape=[1,-1])\n",
    "            pol = self(inputs=(x_aggr, x_idio), training=True)\n",
    "            kp[agent_id].assign(pol[0,0])\n",
    "            mup[agent_id].assign(pol[0,1])\n",
    "            c[agent_id].assign(pol[0,2])\n",
    "            lambdap[agent_id].assign(pol[0,3])\n",
    "        return kp, mup, c, lambdap\n",
    "\n",
    "    @tf.function\n",
    "    def residuals(self, k:Tensor, eps:Tensor, z:float):\n",
    "        N = k.shape[0]\n",
    "        K = tf.math.reduce_sum(k)\n",
    "        Y,r,w = firm(K,z)\n",
    "\n",
    "        # 1st forward pass\n",
    "        kp, mup, c, lambdap = self.forward(k, eps, z)\n",
    "        C = tf.math.reduce_sum(c)\n",
    "        Kp = tf.math.reduce_sum(kp)\n",
    "        BUDGET_RES = budget_residual(k, c, eps, kp, r, w)\n",
    "        CSK_RES = mup*kp\n",
    "        CSC_RES = c*lambdap\n",
    "\n",
    "        # For each possible value of next-period exogenous states, compute the next-period policy\n",
    "        kpp = tf.zeros((N,2,2))\n",
    "        Kpp = np.zeros((2,2))\n",
    "        mupp = np.zeros((N,2,2))\n",
    "        lambdapp = np.zeros((N,2,2))\n",
    "        cp = np.zeros((N,2,2))\n",
    "        Cp = np.zeros((2,2))\n",
    "        ee_comp = np.zeros((N,2,2))\n",
    "        # BUDGET_RES_COND = np.zeros((N,2,2))\n",
    "        # CSK_RES_COND = np.zeros((N,2,2))\n",
    "        # CSC_RES_COND = np.zeros((N,2,2))\n",
    "        # MC_RES_COND = np.zeros((N,2,2))\n",
    "\n",
    "        for zp_id, zp in enumerate(Z):\n",
    "            Yp,rp,wp = firm(Kp,zp)\n",
    "            for epsp_id, epsp in enumerate(E):\n",
    "                ypp = self.forward(kp, epsp, zp)\n",
    "                kpp[:,zp_id,epsp_id] = ypp[0]\n",
    "                mupp[:,zp_id,epsp_id] = ypp[1]\n",
    "                cp[:,zp_id,epsp_id] = ypp[2]\n",
    "                lambdapp[:,zp_id,epsp_id] = ypp[3]\n",
    "                ee_comp[:,zp_id,epsp_id] = tf.pow(c/cp[zp_id,epsp_id],γ)\n",
    "                # BUDGET_RES_COND[:,zp_id,epsp_id] = budget_residual(kp, ypp[2], epsp, ypp[0], rp, wp)\n",
    "                # CSK_RES_COND[:,zp_id,epsp_id] = mupp[:,zp_id,epsp_id]*kpp[:,zp_id,epsp_id]\n",
    "                # CSC_RES_COND[:,zp_id,epsp_id] = cp[:,zp_id,epsp_id]*lambdapp[:,zp_id,epsp_id]\n",
    "\n",
    "                Kpp[zp_id,epsp_id] = tf.math.reduce_sum(kpp[:,zp_id,epsp_id])\n",
    "                Cp[zp_id,epsp_id] = tf.math.reduce_sum(cp[:,zp_id,epsp_id])\n",
    "                # MC_RES_COND[zp_id,epsp_id] = Cp[zp_id,epsp_id] + Kpp[zp_id,epsp_id] - (1-δ)*Kp - Yp\n",
    "        \n",
    "        ee_comp = tf.transpose(ee_comp, [1, 0, 2])\n",
    "        EE_RES = tf.pow(β*(1-δ)*tf.math.reduce_mean(tf.tensordot(P_z[int(z==z_h)],ee_comp), axis=1) - (lambdap - mup), -1./nu) - 1\n",
    "        MC_RES = C + Kp - (1-δ)*K - Y\n",
    "        return BUDGET_RES, CSK_RES, CSC_RES, MC_RES\n",
    "\n",
    "    @tf.function\n",
    "    def train_step(self, data, batch_size):\n",
    "        z, k, eps = data\n",
    "        ERR = 0\n",
    "        for per_id in range(batch_size):\n",
    "            print(\"Input in train_step: \", z, k ,eps)\n",
    "            y = self.forward(z, k, eps)\n",
    "            BUDGET_RES, CSK_RES, CSC_RES, MC_RES = self.residuals(k, eps, z)\n",
    "            ERR += (1./batch_size)*tf.math.reduce_mean(BUDGET_RES*BUDGET_RES + CSK_RES*CSK_RES + CSC_RES*CSC_RES) + MC_RES*MC_RES\n",
    "            k = y[0]\n",
    "            eps = draw_eps(N)\n",
    "            z = draw_z(z)\n",
    "        return ERR, z, k, eps"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compact network approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/slebst/opt/miniconda3/lib/python3.9/site-packages/keras/initializers/initializers_v2.py:120: UserWarning: The initializer GlorotUniform is unseeded and being called multiple times, which will return identical values  each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "n_aggr_repr = 8\n",
    "\n",
    "## AGGREGATE REPRESENTATION UNITS\n",
    "# Common network processes distribution-relevant information\n",
    "x_aggr = Input(shape=(N+1, ), name='Distr-In')\n",
    "#xn_aggr = BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True,)(x_aggr)\n",
    "aggr_1 = Dense(units=4*n_aggr_repr, activation = 'tanh', kernel_initializer=initializer, name='Distr-Dense1')(x_aggr)\n",
    "aggr_2 = Dense(units=4*n_aggr_repr, activation = 'tanh', kernel_initializer=initializer, name='Distr-Dense2')(aggr_1)\n",
    "aggr_3 = Dense(units=2*n_aggr_repr, activation = 'tanh', kernel_initializer=initializer, name='Distr-Dense3')(aggr_2)\n",
    "aggr_4 = Dense(units=n_aggr_repr, activation = 'tanh', kernel_initializer=initializer, name='Distr-Dense4')(aggr_3)\n",
    "\n",
    "## POLICY UNITS\n",
    "# Agent-specific policy units\n",
    "x_idio = Input(shape = (2, ), name='Idio-In')\n",
    "combined = Concatenate(name='Intermediate_Input')([aggr_4, x_idio])\n",
    "interp_c_h_1 = Dense(units=32, input_dim=2+n_aggr_repr, activation = 'tanh', kernel_initializer=initializer, name='Policy-Dense1')(combined)\n",
    "interp_c_h_2 = Dense(units=32, activation = 'tanh', kernel_initializer=initializer, name='Policy-Dense2')(interp_c_h_1)\n",
    "policy = Dense(units=4, activation = 'tanh', kernel_initializer=initializer, name='Policy-Dense3')(interp_c_h_2)\n",
    "\n",
    "nn1 = DENModel(inputs = [x_aggr,x_idio], outputs= policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"den_model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " Distr-In (InputLayer)          [(None, 101)]        0           []                               \n",
      "                                                                                                  \n",
      " Distr-Dense1 (Dense)           (None, 32)           3264        ['Distr-In[0][0]']               \n",
      "                                                                                                  \n",
      " Distr-Dense2 (Dense)           (None, 32)           1056        ['Distr-Dense1[0][0]']           \n",
      "                                                                                                  \n",
      " Distr-Dense3 (Dense)           (None, 16)           528         ['Distr-Dense2[0][0]']           \n",
      "                                                                                                  \n",
      " Distr-Dense4 (Dense)           (None, 8)            136         ['Distr-Dense3[0][0]']           \n",
      "                                                                                                  \n",
      " Idio-In (InputLayer)           [(None, 2)]          0           []                               \n",
      "                                                                                                  \n",
      " Intermediate_Input (Concatenat  (None, 10)          0           ['Distr-Dense4[0][0]',           \n",
      " e)                                                               'Idio-In[0][0]']                \n",
      "                                                                                                  \n",
      " Policy-Dense1 (Dense)          (None, 32)           352         ['Intermediate_Input[0][0]']     \n",
      "                                                                                                  \n",
      " Policy-Dense2 (Dense)          (None, 32)           1056        ['Policy-Dense1[0][0]']          \n",
      "                                                                                                  \n",
      " Policy-Dense3 (Dense)          (None, 4)            132         ['Policy-Dense2[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 6,524\n",
      "Trainable params: 6,524\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "nn1.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brute-force MC-Distribution-processing neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_aggr_reprb = 8\n",
    "\n",
    "## AGGREGATE REPRESENTATION UNITS\n",
    "## Common network processes distribution-relevant information\n",
    "# x_aggrb = Input(shape = (None,2))\n",
    "# xn_aggrb = BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True,)(x_aggr)\n",
    "# aggr_1b = Dense(4*n_aggr_reprb, activation = 'tanh', kernel_initializer=initializer)(xn_aggr)\n",
    "# aggr_2b = Dense(4*n_aggr_reprb, activation = 'tanh', kernel_initializer=initializer)(aggr_1)\n",
    "# aggr_3b = Dense(2*n_aggr_reprb, activation = 'tanh', kernel_initializer=initializer)(aggr_2)\n",
    "# aggr_4b = Dense(n_aggr_reprb, activation = 'tanh', kernel_initializer=initializer)(aggr_3)\n",
    "\n",
    "## POLICY UNITS\n",
    "# Agent-specific policy units\n",
    "# x_idiob = Input(shape = (None,2*N))\n",
    "# interp_c_h_1b = Dense(32, input_dim=2*N+n_aggr_reprb, activation = 'tanh', kernel_initializer=initializer)(tf.concat(values=(x_idiob,aggr_4b),axis=1))\n",
    "# interp_c_h_2b = Dense(32, activation = 'tanh', kernel_initializer=initializer)(interp_c_h_1b)\n",
    "# policyb = Dense(2*N, activation = 'tanh', kernel_initializer=initializer)(interp_c_h_2b)\n",
    "\n",
    "# nn2 = DENModel(inputs = tf.concat(values=(x_aggrb,x_idiob)), outputs= policyb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_k(N: int):\n",
    "    k_ss = 2.\n",
    "    return tf.Variable(tf.random.uniform(shape=[N,], minval=0.7*k_ss, maxval=1.3*k_ss))\n",
    "\n",
    "def train(nn: DENModel, N: int=100, n_epochs=1000, batch_size: int=64):\n",
    "    z = z_h\n",
    "    eps = draw_eps(N)\n",
    "    k = initialize_k(N)\n",
    "    metrics = {'mse': []}\n",
    "\n",
    "    for epoch in tqdm(range(n_epochs)):\n",
    "        with tf.GradientTape() as tape:\n",
    "            ERR, z, k, eps = nn.train_step([z, k, eps], batch_size)\n",
    "        grads = tape.gradient(ERR, nn.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, nn.trainable_weights))\n",
    "        metrics['mse'].append(ERR)\n",
    "\n",
    "        # Log every 200 batches.\n",
    "        if epoch % 100 == 0:\n",
    "            print(\n",
    "                \"Training loss (for one batch) at epoch %d: %.4f\"\n",
    "                % (epoch, float(ERR))\n",
    "            )\n",
    "            print(\"Total # time iterations: %d\" % (batch_size*(1+epoch)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input in train_step:  Tensor(\"data:0\", shape=(), dtype=float32) Tensor(\"data_1:0\", shape=(100,), dtype=float32) Tensor(\"data_2:0\", shape=(100,), dtype=float32)\n",
      "x_aggr:  Tensor(\"concat:0\", shape=(101,), dtype=float32)\n",
      "x_idio:  Tensor(\"concat_1:0\", shape=(2,), dtype=float32)\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 101) for input KerasTensor(type_spec=TensorSpec(shape=(None, 101), dtype=tf.float32, name='Distr-In'), name='Distr-In', description=\"created by layer 'Distr-In'\"), but it was called on an input with incompatible shape (101,).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 2) for input KerasTensor(type_spec=TensorSpec(shape=(None, 2), dtype=tf.float32, name='Idio-In'), name='Idio-In', description=\"created by layer 'Idio-In'\"), but it was called on an input with incompatible shape (2,).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/var/folders/jz/7fj0cjjx1zbbj_0k1f_617h40000gn/T/ipykernel_15185/3205358969.py\", line 91, in train_step  *\n        y = self.forward(z, k, eps)\n    File \"/var/folders/jz/7fj0cjjx1zbbj_0k1f_617h40000gn/T/ipykernel_15185/3205358969.py\", line 29, in forward  *\n        pol = self(inputs=(x_aggr, x_idio), training=True)\n    File \"/Users/slebst/opt/miniconda3/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"/Users/slebst/opt/miniconda3/lib/python3.9/site-packages/keras/engine/input_spec.py\", line 250, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Exception encountered when calling layer 'den_model_1' (type DENModel).\n    \n    Input 0 of layer \"Distr-Dense1\" is incompatible with the layer: expected min_ndim=2, found ndim=1. Full shape received: (101,)\n    \n    Call arguments received by layer 'den_model_1' (type DENModel):\n      • inputs=('tf.Tensor(shape=(101,), dtype=float32)', 'tf.Tensor(shape=(2,), dtype=float32)')\n      • training=True\n      • mask=None\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train(nn1, N\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[15], line 13\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(nn, N, n_epochs, batch_size)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(n_epochs)):\n\u001b[1;32m     12\u001b[0m     \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mGradientTape() \u001b[39mas\u001b[39;00m tape:\n\u001b[0;32m---> 13\u001b[0m         ERR, z, k, eps \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39;49mtrain_step([z, k, eps], batch_size)\n\u001b[1;32m     14\u001b[0m     grads \u001b[39m=\u001b[39m tape\u001b[39m.\u001b[39mgradient(ERR, nn\u001b[39m.\u001b[39mtrainable_weights)\n\u001b[1;32m     15\u001b[0m     optimizer\u001b[39m.\u001b[39mapply_gradients(\u001b[39mzip\u001b[39m(grads, nn\u001b[39m.\u001b[39mtrainable_weights))\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m    154\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/var/folders/jz/7fj0cjjx1zbbj_0k1f_617h40000gn/T/__autograph_generated_filehn2malpq.py:37\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_step\u001b[0;34m(self, data, batch_size)\u001b[0m\n\u001b[1;32m     35\u001b[0m per_id \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mUndefined(\u001b[39m'\u001b[39m\u001b[39mper_id\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     36\u001b[0m y \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mUndefined(\u001b[39m'\u001b[39m\u001b[39my\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 37\u001b[0m ag__\u001b[39m.\u001b[39mfor_stmt(ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mrange\u001b[39m), (ag__\u001b[39m.\u001b[39mld(batch_size),), \u001b[39mNone\u001b[39;00m, fscope), \u001b[39mNone\u001b[39;00m, loop_body, get_state, set_state, (\u001b[39m'\u001b[39m\u001b[39mERR\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39meps\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mk\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mz\u001b[39m\u001b[39m'\u001b[39m), {\u001b[39m'\u001b[39m\u001b[39miterate_names\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mper_id\u001b[39m\u001b[39m'\u001b[39m})\n\u001b[1;32m     38\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     39\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/var/folders/jz/7fj0cjjx1zbbj_0k1f_617h40000gn/T/__autograph_generated_filehn2malpq.py:24\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_step.<locals>.loop_body\u001b[0;34m(itr)\u001b[0m\n\u001b[1;32m     22\u001b[0m per_id \u001b[39m=\u001b[39m itr\n\u001b[1;32m     23\u001b[0m ag__\u001b[39m.\u001b[39mld(\u001b[39mprint\u001b[39m)(\u001b[39m'\u001b[39m\u001b[39mInput in train_step: \u001b[39m\u001b[39m'\u001b[39m, ag__\u001b[39m.\u001b[39mld(z), ag__\u001b[39m.\u001b[39mld(k), ag__\u001b[39m.\u001b[39mld(eps))\n\u001b[0;32m---> 24\u001b[0m y \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39;49mconverted_call(ag__\u001b[39m.\u001b[39;49mld(\u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mforward, (ag__\u001b[39m.\u001b[39;49mld(z), ag__\u001b[39m.\u001b[39;49mld(k), ag__\u001b[39m.\u001b[39;49mld(eps)), \u001b[39mNone\u001b[39;49;00m, fscope)\n\u001b[1;32m     25\u001b[0m (BUDGET_RES, CSK_RES, CSC_RES, MC_RES) \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mresiduals, (ag__\u001b[39m.\u001b[39mld(k), ag__\u001b[39m.\u001b[39mld(eps), ag__\u001b[39m.\u001b[39mld(z)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[1;32m     26\u001b[0m ERR \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mld(ERR)\n",
      "File \u001b[0;32m/var/folders/jz/7fj0cjjx1zbbj_0k1f_617h40000gn/T/__autograph_generated_fileb7cyz5me.py:37\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__forward\u001b[0;34m(self, z, k, eps)\u001b[0m\n\u001b[1;32m     35\u001b[0m x_idio \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mUndefined(\u001b[39m'\u001b[39m\u001b[39mx_idio\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     36\u001b[0m pol \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mUndefined(\u001b[39m'\u001b[39m\u001b[39mpol\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 37\u001b[0m ag__\u001b[39m.\u001b[39mfor_stmt(ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mrange\u001b[39m), (ag__\u001b[39m.\u001b[39mld(N),), \u001b[39mNone\u001b[39;00m, fscope), \u001b[39mNone\u001b[39;00m, loop_body, get_state, set_state, (), {\u001b[39m'\u001b[39m\u001b[39miterate_names\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39magent_id\u001b[39m\u001b[39m'\u001b[39m})\n\u001b[1;32m     38\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     39\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/var/folders/jz/7fj0cjjx1zbbj_0k1f_617h40000gn/T/__autograph_generated_fileb7cyz5me.py:29\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__forward.<locals>.loop_body\u001b[0;34m(itr)\u001b[0m\n\u001b[1;32m     27\u001b[0m x_idio \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(tf)\u001b[39m.\u001b[39mconcat, ([ag__\u001b[39m.\u001b[39mld(k)[ag__\u001b[39m.\u001b[39mld(agent_id)][\u001b[39mNone\u001b[39;00m], ag__\u001b[39m.\u001b[39mld(eps)[ag__\u001b[39m.\u001b[39mld(agent_id)][\u001b[39mNone\u001b[39;00m]],), \u001b[39mdict\u001b[39m(axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m), fscope)\n\u001b[1;32m     28\u001b[0m ag__\u001b[39m.\u001b[39mld(\u001b[39mprint\u001b[39m)(\u001b[39m'\u001b[39m\u001b[39mx_idio: \u001b[39m\u001b[39m'\u001b[39m, ag__\u001b[39m.\u001b[39mld(x_idio))\n\u001b[0;32m---> 29\u001b[0m pol \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39;49mconverted_call(ag__\u001b[39m.\u001b[39;49mld(\u001b[39mself\u001b[39;49m), (), \u001b[39mdict\u001b[39;49m(inputs\u001b[39m=\u001b[39;49m(ag__\u001b[39m.\u001b[39;49mld(x_aggr), ag__\u001b[39m.\u001b[39;49mld(x_idio)), training\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m), fscope)\n\u001b[1;32m     30\u001b[0m ag__\u001b[39m.\u001b[39mld(kp)[ag__\u001b[39m.\u001b[39mld(agent_id)] \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mld(pol)[\u001b[39m0\u001b[39m]\n\u001b[1;32m     31\u001b[0m ag__\u001b[39m.\u001b[39mld(mup)[ag__\u001b[39m.\u001b[39mld(agent_id)] \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mld(pol)[\u001b[39m1\u001b[39m]\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/keras/engine/input_spec.py:250\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    248\u001b[0m     ndim \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mshape\u001b[39m.\u001b[39mrank\n\u001b[1;32m    249\u001b[0m     \u001b[39mif\u001b[39;00m ndim \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m ndim \u001b[39m<\u001b[39m spec\u001b[39m.\u001b[39mmin_ndim:\n\u001b[0;32m--> 250\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    251\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mInput \u001b[39m\u001b[39m{\u001b[39;00minput_index\u001b[39m}\u001b[39;00m\u001b[39m of layer \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mlayer_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    252\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mis incompatible with the layer: \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    253\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mexpected min_ndim=\u001b[39m\u001b[39m{\u001b[39;00mspec\u001b[39m.\u001b[39mmin_ndim\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    254\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mfound ndim=\u001b[39m\u001b[39m{\u001b[39;00mndim\u001b[39m}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    255\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFull shape received: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtuple\u001b[39m(shape)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    256\u001b[0m         )\n\u001b[1;32m    257\u001b[0m \u001b[39m# Check dtype.\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[39mif\u001b[39;00m spec\u001b[39m.\u001b[39mdtype \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/var/folders/jz/7fj0cjjx1zbbj_0k1f_617h40000gn/T/ipykernel_15185/3205358969.py\", line 91, in train_step  *\n        y = self.forward(z, k, eps)\n    File \"/var/folders/jz/7fj0cjjx1zbbj_0k1f_617h40000gn/T/ipykernel_15185/3205358969.py\", line 29, in forward  *\n        pol = self(inputs=(x_aggr, x_idio), training=True)\n    File \"/Users/slebst/opt/miniconda3/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"/Users/slebst/opt/miniconda3/lib/python3.9/site-packages/keras/engine/input_spec.py\", line 250, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Exception encountered when calling layer 'den_model_1' (type DENModel).\n    \n    Input 0 of layer \"Distr-Dense1\" is incompatible with the layer: expected min_ndim=2, found ndim=1. Full shape received: (101,)\n    \n    Call arguments received by layer 'den_model_1' (type DENModel):\n      • inputs=('tf.Tensor(shape=(101,), dtype=float32)', 'tf.Tensor(shape=(2,), dtype=float32)')\n      • training=True\n      • mask=None\n"
     ]
    }
   ],
   "source": [
    "train(nn1, N=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = initialize_k(N)\n",
    "eps = draw_eps(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 4), dtype=float32, numpy=\n",
       "array([[ 0.63211966, -0.26554337, -0.14226943,  0.8057285 ]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_aggr = tf.concat([z_h[None],k],axis=0)\n",
    "x_aggr = tf.reshape(x_aggr, shape=[1,-1])\n",
    "\n",
    "x_idio = tf.concat([k[0][None], eps[0][None]], axis=0)\n",
    "x_idio = tf.reshape(x_idio, shape=[1,-1])\n",
    "\n",
    "nn1(inputs=(x_aggr, x_idio), training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "kp, mup, c, lambdap = nn1.forward(z_h,k,eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Dynamic Programming & DL",
   "language": "python",
   "name": "dynprog_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
