{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving Krussel-Smith with Deep Equilibrium Nets\n",
    "*Simon Lebastard, 02/07/2023*\n",
    "\n",
    "## Krussel-Smith (1998) model\n",
    "In the KS'98 model, we look a flavour of the stochastic growth model with a continuum of households under partially uninsurable risk.\n",
    "\n",
    "**Demand side**\n",
    "$$V(c) = \\mathrm{E}_0\\Big[\\sum_{t=0}^{\\infty}{\\beta^t U(c_t)}\\Big]$$\n",
    "with\n",
    "$$U(c) = \\lim_{\\nu \\to \\sigma}{\\frac{c^{1-\\nu} - 1}{1 - \\nu}}$$\n",
    "\n",
    "Agents are each endowed with $\\epsilon \\tilde{l}$ units of labor per period, where $\\epsilon$ is a first-order Markov chain. (In K&S'98, $\\epsilon$ can only take two values: 0 and 1, representing unemployed and employed idiosyncratic states, respectively).\n",
    "As a very large number of agents is consided, and by assumed independence of the idiosyncratic shocks, K&S'98 assume that the total number of employed and unemployed people remain constant over time. That is, there is no aggregate fluctuations of jobs supply.\n",
    "\n",
    "Idiosyncratic endogenous state: $k$ holding of capital\n",
    "Idiosyncratic exogenous state: $\\epsilon$ at the individual level\n",
    "$\\Gamma$ the joint distribution of idiosyncratic states $(k,\\epsilon)$\n",
    "\n",
    "Given the aggregate states (see $z$ defined below), the consumer's optimization problem is:\n",
    "$$v(k,\\epsilon;\\Gamma,z) = \\max_{c,k'}{ \\Big\\{ U(c) + \\beta\\mathrm{E}\\Big[v(k',\\epsilon';\\Gamma',z') \\mid z,\\epsilon \\Big] \\Big\\} } $$\n",
    "under budget constraint, rational expectations wrt law of motion and non-negative capital holding.\n",
    "\n",
    "The budget constraint writes:\n",
    "$$c+k' = r(\\hat{k},\\hat{l},z)k + w(\\hat{k},\\hat{l},z)\\tilde{l}\\epsilon + (1-\\delta)k$$\n",
    "\n",
    "**Supply side**\n",
    "A single-type good is produced using two factors of production: labor $l$ and capital $k$.\n",
    "The good is produced according to a Cobb-Douglas production function:\n",
    "$$y = zk^{\\alpha}l^{1-\\alpha}, \\quad \\alpha \\in [0,1]$$\n",
    "The TFP is stochastic and coresponds to the source of aggregate risk. In K&S'98, two aggregate states are considered: $(z_b, z_g)$.\n",
    "Again, $z$ follows a first-order Markov chain.\n",
    "\n",
    "We assume the production market to be competitive, such that wages $w$ and rental rates $r$, both functions of aggregate states, are respectively determined by:\n",
    "$$w(\\hat{k},\\hat{l},z) = (1 - \\alpha)z(\\frac{\\hat{k}}{\\hat{l}})^\\alpha$$\n",
    "$$r(\\hat{k},\\hat{l},z) = \\alpha z(\\frac{\\hat{k}}{\\hat{l}})^{\\alpha-1}$$\n",
    "\n",
    "Here we assume that as the population of households is infinitely large, the share of unemployed remains constant. Moreover, here consumers have no disutility from labor, implying that the labor supply at each period is constant at $L_s = N*\\mathrm{E}\\big[\\epsilon\\big]$.\n",
    "We have the market clearing condition for the capital/cons good: $$\\int{(c(k,\\epsilon;\\Gamma,z) + k'(k,\\epsilon;\\Gamma,z))dF(\\Gamma)} = (1-\\delta)\\int{k dF(\\Gamma)} + z\\int{k dF(\\Gamma)}^{\\alpha}L_s^{1-\\alpha}$$\n",
    "\n",
    "For both Markov chains, we assume the economy is already running at the stationary distribution.\n",
    "\n",
    "**Law of motion**\n",
    "$$\\Gamma' = H(\\Gamma,z,z')$$\n",
    "\n",
    "\n",
    "### Formulating the problem for solving with DEN\n",
    "Here we will consider two \"implicit\" policy functions for which we solve:\n",
    "- Next-period capital $k'(k,\\epsilon;\\Gamma,z)$\n",
    "- The Lagrange multiplier on the next-period capital non-negativity constraint: $\\mu_k(k,\\epsilon;\\Gamma,z)$\n",
    "- The Lagrange multiplier on the positivity of current-period consumption, ie of the residual of the budget constraint: $\\mu_c(k,\\epsilon;\\Gamma,z)$\n",
    "\n",
    "As in Azinovic et al, we simulate $N$ agents, and index $i \\in [1,...,N]$.\n",
    "\n",
    "#### Idiosyncratic error terms\n",
    "The Euler equation can be obtained by taking the FOC of the consumer's objective function with respect to next-period capital:\n",
    "$$1 = \\beta(1-\\delta)\\mathrm{E}\\Big[\\Big(\\frac{c(k,\\epsilon;\\Gamma,z)}{c(k',\\epsilon';\\Gamma',z')}\\Big)^{\\nu} \\mid z,\\epsilon\\Big] - \\big(\\mu_c(k,\\epsilon;\\Gamma,z) - \\mu_k(k,\\epsilon;\\Gamma,z) \\big)c(k,\\epsilon;\\Gamma,z)^{\\nu}$$\n",
    "\n",
    "Based on that, we defined the Euler error as:\n",
    "$$e_{EE}(k,\\epsilon,\\Gamma,z) \\equiv \\bigg[\\beta(1-\\delta)\\mathrm{E}\\Big[\\Big(\\frac{c(k,\\epsilon;\\Gamma,z)}{c(k',\\epsilon';\\Gamma',z')}\\Big)^{\\nu} \\mid z,\\epsilon\\Big] - \\big(\\mu_c(k,\\epsilon;\\Gamma,z) - \\mu_k(k,\\epsilon;\\Gamma,z) \\big)\\bigg]^{-\\frac{1}{\\nu}} - 1$$\n",
    "and\n",
    "$$e_{EE,i} \\equiv e_{EE}(k_i,\\epsilon_i,\\Gamma,z)$$\n",
    "Here we want to train the model to capture binding constraints by itself. Note that in practice, during training we may end up with negative consumptions (that will be penalized), such that ratio $\\frac{c(k,\\epsilon;\\Gamma,z)}{c(k',\\epsilon';\\Gamma',z')}^\\nu$ may not be defined for $\\nu \\in \\mathbb{R}$.\n",
    "To solve this issue, I use RELU activation in the last two layers of the network, to enforce $c(k,\\epsilon;\\Gamma,z), k'(k,\\epsilon;\\Gamma,z), \\mu_c(k,\\epsilon;\\Gamma,z), \\mu_k(k,\\epsilon;\\Gamma,z)$ to be non-negative for all states.\n",
    "\n",
    "We define the error on the complementary-slackness condition on k as:\n",
    "$$e_{CS_k}(k,\\epsilon,\\Gamma,z) \\equiv \\frac{\\mu_k(k,\\epsilon;\\Gamma,z)}{U'(\\bar{c})} \\frac{k'(k,\\epsilon;\\Gamma,z)}{\\bar{k}}$$\n",
    "and\n",
    "$$e_{CS_k,i} \\equiv e_{CS_k}(k_i,\\epsilon_i,\\Gamma,z)$$\n",
    "\n",
    "We define the error on the complementary-slackness condition on c as:\n",
    "$$e_{CS_c}(k,\\epsilon,\\Gamma,z) \\equiv \\frac{\\mu_c(k,\\epsilon;\\Gamma,z)}{U'(\\bar{c})} \\frac{c(k,\\epsilon;\\Gamma,z)}{\\bar{c}}$$\n",
    "and\n",
    "$$e_{CS_c,i} \\equiv e_{CS_c}(k_i,\\epsilon_i,\\Gamma,z)$$\n",
    "\n",
    "The agent's budget constraint is:\n",
    "$$e_{BC}(k,\\epsilon,\\Gamma,z) \\equiv \\frac{1}{\\hat{c}+\\hat{k}}\\bigg[ c + k' - \\big( 1 + r(\\hat{k}, \\hat{l}, z) - \\delta \\big)k - w(\\hat{k}, \\hat{l}, z)\\tilde{l}\\epsilon \\bigg] $$\n",
    "\n",
    "#### Aggregate error term\n",
    "At each period, we compute the aggregate $K \\equiv \\sum_{i=1}^{N}{k_i}$\n",
    "\n",
    "One way to proceed could be to define an error based on the market clearing condition on capital. Instead, we will enforce consumption to satisfy the market clearing condition at each period. Note that in doing so, we could still need to enforce that consumption is positive. Instead of enforcing a new constraint on a Lagrangian multiplier associated with consumption, I compute the error on MC by constraining consumption to be positive in the error, by using a transformation of the error that satisfies:\n",
    "$$\\lim_{c \\downarrow 0}{e_{MC}(c)} = \\infty$$\n",
    "This should prevent consumption from ever being non-positive.\n",
    "\n",
    "We define the error on the market clearing condition as:\n",
    "$$e_{MC} \\equiv \\sum_{i=1}^{N}{\\Big[c(k_i,\\epsilon_i;\\Gamma,z) + k'(k_i,\\epsilon_i;\\Gamma,z)\\Big]} - zK^{\\alpha}(Nu)^{1-\\alpha} - (1-\\delta)K$$\n",
    "\n",
    "We also define the error on the aggregate law of motion as:\n",
    "$$e_{LM} \\equiv $$\n",
    "\n",
    "#### Defining the loss function\n",
    "On a batch $\\mathcal{D}_{train}$, the loss function is defined as:\n",
    "$$l(\\theta) \\equiv \\frac{1}{\\mid\\mathcal{D}_{train}\\mid} \\sum_{x \\in \\mathcal{D}_{train}}{\\frac{1}{N-1}\\sum_{i=1}^{N}{\\Big(e_{EE,i}^2 + e_{CS_k,i}^2 + e_{CS_c,i}^2\\Big)} + e_{MC}^2}$$\n",
    "\n",
    "**Note: alternative implementation**<br>\n",
    "Instead of having a policy variable for the Lagrange multiplier on the positivity of next-period capital, we could have a transformation function on  next-period capital that ensures it remains non-negative at all times. I will implement this alternative method and compare results.\n",
    "\n",
    "#### Network architecture\n",
    "The network architecture is composite, with one subnetwork tasked with learning a useful representation of the distribution of capital holdings, and a second agent-network tasked with learning agent's policy on next-period capital and consumption.\n",
    "\n",
    "**Distribution representation network**\n",
    "- $1+N$ scalar inputs: $(z, \\big\\{k_i\\big\\}_{i \\in [1..N]})$\n",
    "- $n_{repr,distr}$ scalar outputs\n",
    "\n",
    "**Policy network**\n",
    "- $n_{repr,distr} + 2$ scalar inputs, with agent-specific inputs being $(\\epsilon_i, k_i)$\n",
    "- 4 scalar outputs: $(k_i', {\\mu_{k}}_i, c_i, {\\mu_{c}}_i)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-10 16:54:05.533857: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Import modules\n",
    "import os\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.config.run_functions_eagerly(True)\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from keras.models import Model\n",
    "from keras.layers import * #Input, Dense, BatchNormalization\n",
    "from tensorflow import Tensor\n",
    "\n",
    "# Set the seed for replicable results\n",
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "#tf.set_random_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def firm(K:Tensor, z:Tensor):\n",
    "    prod = z*tf.pow(K, α)*tf.pow(L, 1-α)\n",
    "    r = z*α*tf.pow(K, α-1)*tf.pow(L, 1-α)\n",
    "    w = z*(1-α)*tf.pow(K, α)*tf.pow(L, -α)\n",
    "    return prod, r, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-10 16:54:10.102653: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.random import stateless_binomial\n",
    "\n",
    "# Shocks structure\n",
    "Z = tf.constant([0.9, 1.1], dtype=tf.float32)\n",
    "z_l = Z[0]\n",
    "z_h = Z[1]\n",
    "P_z = tf.constant([[0.9, 0.1], [0.1, 0.9]], dtype=tf.float32)\n",
    "def draw_z(z):\n",
    "    z_id = int(z==z_h)\n",
    "    zp_id = stateless_binomial(shape=[1], seed=[123, 456], counts=[1.], probs=[P_z[z_id,1]])[0]\n",
    "    return Z[zp_id]\n",
    "\n",
    "E = tf.constant([0, 1], dtype=tf.float32)\n",
    "eps_l = E[0]\n",
    "eps_h = E[1]\n",
    "P_eps = tf.constant([[0.5, 0.5], [0.5, 0.5]], dtype=tf.float32)\n",
    "def draw_eps(N: int):\n",
    "    return stateless_binomial(\n",
    "        shape=[N,],\n",
    "        seed=[123, 456],\n",
    "        counts=1,\n",
    "        probs=0.5,\n",
    "        output_dtype=tf.float32,\n",
    "    )\n",
    "\n",
    "# Other constants\n",
    "α = tf.constant(0.3, dtype=tf.float32)\n",
    "β = tf.constant(0.7, dtype=tf.float32)\n",
    "δ = tf.constant(0.1, dtype=tf.float32)\n",
    "γ = tf.constant(2.0, dtype=tf.float32)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network - Architecture\n",
    "\n",
    "The following is a specialization class of Keras' Model, with a custom training step with gradient taping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def budget_residual(k: Tensor, c: Tensor, eps:Tensor, kp: Tensor, r:float, w:float):\n",
    "    return w*eps + (1+r-δ)*k - kp - c\n",
    "\n",
    "@tf.function\n",
    "def FB(a:float, b:float):\n",
    "    return a + b - tf.sqrt(tf.pow(a,2) + tf.pow(b,2))\n",
    "\n",
    "class DENModel(Model):\n",
    "\n",
    "    @tf.function\n",
    "    def __init__(self, inputs, *args, **kwargs):\n",
    "        self.N = inputs[0].shape[1] - 1\n",
    "        super().__init__(inputs, *args, **kwargs)\n",
    "\n",
    "    @tf.function\n",
    "    def initialize_k(self):\n",
    "        k_ss = 2.\n",
    "        return tf.Variable(tf.random.uniform(shape=[self.N,], minval=0.8*k_ss, maxval=1.2*k_ss))\n",
    "\n",
    "    @tf.function\n",
    "    def _forward_DEPRECATED(self, z:Tensor, k:Tensor, eps:Tensor, training:bool=False):\n",
    "        kp = tf.Variable(tf.zeros_like(k))\n",
    "        mup = tf.Variable(tf.zeros_like(k))\n",
    "        c = tf.Variable(tf.zeros_like(k))\n",
    "        lambdap = tf.Variable(tf.zeros_like(k))\n",
    "\n",
    "        x_aggr = tf.concat([z[None],k],axis=0)\n",
    "        x_aggr = tf.reshape(x_aggr, shape=[1,-1])\n",
    "        for agent_id in range(self.N):\n",
    "            x_idio = tf.concat([k[agent_id][None], eps[agent_id][None]], axis=0)\n",
    "            x_idio = tf.reshape(x_idio, shape=[1,-1])\n",
    "            pol = self(inputs=(x_aggr, x_idio), training=training)\n",
    "            kp[agent_id].assign(pol[0,0])\n",
    "            mup[agent_id].assign(pol[0,1])\n",
    "            c[agent_id].assign(pol[0,2])\n",
    "            lambdap[agent_id].assign(pol[0,3])\n",
    "        return kp, mup, c, lambdap\n",
    "\n",
    "    @tf.function\n",
    "    def forward(self, z:Tensor, k:Tensor, eps:Tensor, training:bool=False):\n",
    "        x_aggr = tf.concat([z[None],k],axis=0)\n",
    "        x_aggr = tf.tile(tf.reshape(x_aggr, shape=[1,-1]), tf.constant([self.N,1], tf.int32))\n",
    "        \n",
    "        x_idio = tf.concat([tf.reshape(k,[-1,1]),tf.reshape(eps,[-1,1])],axis=1)\n",
    "        y = self(inputs=(x_aggr, x_idio), training=training)\n",
    "        return y\n",
    "\n",
    "    @tf.function\n",
    "    def residuals(self, z:Tensor, k:Tensor, eps:Tensor):\n",
    "        K = tf.math.reduce_sum(k)\n",
    "        if K<0:\n",
    "            print(\"K: NEGATIVE AGGREGATE CAPITAL DESPITE RELU!!\")\n",
    "        Y,r,w = firm(K,z)\n",
    "        #print(\"CURRENT-PERIOD AGGREGATE CAPITAL\")\n",
    "        #print(K)\n",
    "        #print(\"CURRENT-PERIOD PRODUCTION, ROC, WAGE\")\n",
    "        #print(Y,r,w)\n",
    "\n",
    "        # 1st forward pass\n",
    "        yp = self.forward(z, k, eps, training=True)\n",
    "        kp = yp[:,0]\n",
    "        mup = yp[:,1]\n",
    "        c = yp[:,2]\n",
    "        lambdap = yp[:,3]\n",
    "        C = tf.math.reduce_sum(c)\n",
    "        Kp = tf.math.reduce_sum(kp)\n",
    "        if Kp<0:\n",
    "            print(\"Kp: NEGATIVE AGGREGATE CAPITAL DESPITE RELU!!\")\n",
    "        BUDGET_RES = budget_residual(k, c, eps, kp, r, w)\n",
    "        CSK_RES = mup*kp\n",
    "        CSC_RES = c*lambdap\n",
    "\n",
    "        # For each possible value of next-period exogenous states, compute the next-period policy\n",
    "        kpp = tf.Variable(tf.zeros((self.N,2)))\n",
    "        Kpp = tf.Variable(tf.zeros((2,)))\n",
    "        mupp = tf.Variable(tf.zeros((self.N,2)))\n",
    "        lambdapp = tf.Variable(tf.zeros((self.N,2)))\n",
    "        cp = tf.Variable(tf.zeros((self.N,2)))\n",
    "        Cp = tf.Variable(tf.zeros((2,)))\n",
    "        ee_comp = tf.Variable(tf.zeros((self.N,2)))\n",
    "        # BUDGET_RES_COND = np.zeros((N,2))\n",
    "        # CSK_RES_COND = np.zeros((N,2))\n",
    "        # CSC_RES_COND = np.zeros((N,2))\n",
    "        # MC_RES_COND = np.zeros((N,2))\n",
    "\n",
    "        for zp_id, zp in enumerate(Z):\n",
    "            Yp,rp,wp = firm(Kp,zp)\n",
    "            epsp = draw_eps(self.N)\n",
    "            ypp = self.forward(zp, kp, epsp, training=True)\n",
    "            kpp[:,zp_id].assign(ypp[:,0])\n",
    "            mupp[:,zp_id].assign(ypp[:,1])\n",
    "            cp[:,zp_id].assign(ypp[:,2])\n",
    "            lambdapp[:,zp_id].assign(ypp[:,3])\n",
    "            ee_comp_tmp = tf.pow(c/cp[:,zp_id],γ)\n",
    "            ee_comp[:,zp_id].assign(tf.where(tf.math.is_nan(ee_comp_tmp), 1e6*tf.ones_like(ee_comp_tmp), ee_comp_tmp))\n",
    "            # BUDGET_RES_COND[:,zp_id] = budget_residual(kp, ypp[2], epsp, ypp[0], rp, wp)\n",
    "            # CSK_RES_COND[:,zp_id] = mupp[:,zp_id]*kpp[:,zp_id]\n",
    "            # CSC_RES_COND[:,zp_id] = cp[:,zp_id]*lambdapp[:,zp_id]\n",
    "\n",
    "            Kpp[zp_id].assign(tf.math.reduce_sum(kpp[:,zp_id]))\n",
    "            if Kpp[zp_id]<0:\n",
    "                print(\"Kpp[{0:d}]: NEGATIVE AGGREGATE CAPITAL DESPITE RELU!!\".format(zp_id))\n",
    "            Cp[zp_id].assign(tf.math.reduce_sum(cp[:,zp_id]))\n",
    "            # MC_RES_COND[zp_id] = Cp[zp_id] + Kpp[zp_id] - (1-δ)*Kp - Yp\n",
    "        \n",
    "        EE_RES = tf.pow(\n",
    "            β*(1-δ)*tf.tensordot(ee_comp,tf.transpose(P_z[int(z==z_h),:]), axes=1) - (lambdap - mup),\n",
    "            -1./γ\n",
    "        ) - 1\n",
    "        \n",
    "        MC_RES = C + Kp - (1-δ)*K - Y\n",
    "        return BUDGET_RES, CSK_RES, CSC_RES, MC_RES\n",
    "\n",
    "    @tf.function\n",
    "    def train_step(self, data, batch_size):\n",
    "        z, k, eps = data\n",
    "        ERR = 0\n",
    "        for per_id in range(batch_size):\n",
    "            y = self.forward(z, k, eps, training=False)\n",
    "            BUDGET_RES, CSK_RES, CSC_RES, MC_RES = self.residuals(z, k, eps)\n",
    "            # print(\"BUDGET RESIDUAL: \", BUDGET_RES*BUDGET_RES)\n",
    "            # print(\"CS on K: \", CSK_RES*CSK_RES)\n",
    "            # print(\"CS on C: \", CSC_RES*CSC_RES)\n",
    "            # print(\"MK: \", MC_RES*MC_RES)\n",
    "            # print(\"TOTAL ERROR FORM IS SOURCES: \", tf.math.reduce_mean(BUDGET_RES*BUDGET_RES + CSK_RES*CSK_RES + CSC_RES*CSC_RES))\n",
    "            ERR += (1./batch_size)*tf.math.reduce_mean(BUDGET_RES*BUDGET_RES + CSK_RES*CSK_RES + CSC_RES*CSC_RES) + MC_RES*MC_RES\n",
    "            k = y[:,0]\n",
    "            eps = draw_eps(self.N)\n",
    "            z = draw_z(z)\n",
    "        return ERR, z, k, eps\n",
    "    \n",
    "    @tf.function\n",
    "    def train(self, optimizer, n_epochs=1000, batch_size: int=64):\n",
    "        z = z_h\n",
    "        eps = draw_eps(self.N)\n",
    "        k = self.initialize_k()\n",
    "        metrics = {'mse': []}\n",
    "\n",
    "        for epoch in tqdm(range(n_epochs)):\n",
    "            with tf.GradientTape() as tape:\n",
    "                ERR, z, k, eps = self.train_step([z, k, eps], batch_size)\n",
    "            grads = tape.gradient(ERR, self.trainable_weights)\n",
    "            optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "            metrics['mse'].append(ERR.numpy())\n",
    "\n",
    "            # Log every 200 batches.\n",
    "            if epoch % 10 == 0:\n",
    "                print(\n",
    "                    \"Training loss (for one batch) at epoch %d: %.4f\"\n",
    "                    % (epoch, float(ERR))\n",
    "                )\n",
    "                print(\"Total # time iterations: %d\" % (batch_size*(1+epoch)))\n",
    "            \n",
    "        return metrics\n",
    "    \n",
    "lr = 0.00001\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate=lr,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compact network approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/slebst/opt/miniconda3/lib/python3.9/site-packages/keras/initializers/initializers_v2.py:120: UserWarning: The initializer GlorotUniform is unseeded and being called multiple times, which will return identical values  each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "N = num_agents = 500\n",
    "L = N*tf.reduce_mean(E)\n",
    "n_aggr_repr = 8\n",
    "\n",
    "initializer = tf.keras.initializers.GlorotUniform()\n",
    "\n",
    "## AGGREGATE REPRESENTATION UNITS\n",
    "# Common network processes distribution-relevant information\n",
    "x_aggr = Input(shape=(N+1, ), name='Distr-In')\n",
    "#xn_aggr = BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True,)(x_aggr)\n",
    "aggr_1 = Dense(units=4*n_aggr_repr, activation = 'tanh', kernel_initializer=initializer, name='Distr-Dense1')(x_aggr)\n",
    "aggr_2 = Dense(units=4*n_aggr_repr, activation = 'tanh', kernel_initializer=initializer, name='Distr-Dense2')(aggr_1)\n",
    "aggr_3 = Dense(units=2*n_aggr_repr, activation = 'tanh', kernel_initializer=initializer, name='Distr-Dense3')(aggr_2)\n",
    "aggr_4 = Dense(units=n_aggr_repr, activation = 'tanh', kernel_initializer=initializer, name='Distr-Dense4')(aggr_3)\n",
    "\n",
    "## POLICY UNITS\n",
    "# Agent-specific policy units\n",
    "x_idio = Input(shape = (2, ), name='Idio-In')\n",
    "combined = Concatenate(name='Intermediate_Input')([aggr_4, x_idio])\n",
    "interp_c_h_1 = Dense(units=32, input_dim=2+n_aggr_repr, activation = 'tanh', kernel_initializer=initializer, name='Policy-Dense1')(combined)\n",
    "interp_c_h_2 = Dense(units=32, activation = 'tanh', kernel_initializer=initializer, name='Policy-Dense2')(interp_c_h_1)\n",
    "interp_c_h_3 = Dense(units=32, activation = 'relu', kernel_initializer=initializer, name='Policy-Relu1')(interp_c_h_2)\n",
    "policy = Dense(units=4, activation = 'relu', kernel_initializer=initializer, name='Policy-Relu2')(interp_c_h_3)\n",
    "\n",
    "model = DENModel(inputs = [x_aggr,x_idio], outputs= policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"den_model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " Distr-In (InputLayer)          [(None, 501)]        0           []                               \n",
      "                                                                                                  \n",
      " Distr-Dense1 (Dense)           (None, 32)           16064       ['Distr-In[0][0]']               \n",
      "                                                                                                  \n",
      " Distr-Dense2 (Dense)           (None, 32)           1056        ['Distr-Dense1[0][0]']           \n",
      "                                                                                                  \n",
      " Distr-Dense3 (Dense)           (None, 16)           528         ['Distr-Dense2[0][0]']           \n",
      "                                                                                                  \n",
      " Distr-Dense4 (Dense)           (None, 8)            136         ['Distr-Dense3[0][0]']           \n",
      "                                                                                                  \n",
      " Idio-In (InputLayer)           [(None, 2)]          0           []                               \n",
      "                                                                                                  \n",
      " Intermediate_Input (Concatenat  (None, 10)          0           ['Distr-Dense4[0][0]',           \n",
      " e)                                                               'Idio-In[0][0]']                \n",
      "                                                                                                  \n",
      " Policy-Dense1 (Dense)          (None, 32)           352         ['Intermediate_Input[0][0]']     \n",
      "                                                                                                  \n",
      " Policy-Dense2 (Dense)          (None, 32)           1056        ['Policy-Dense1[0][0]']          \n",
      "                                                                                                  \n",
      " Policy-Relu1 (Dense)           (None, 32)           1056        ['Policy-Dense2[0][0]']          \n",
      "                                                                                                  \n",
      " Policy-Relu2 (Dense)           (None, 4)            132         ['Policy-Relu1[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 20,380\n",
      "Trainable params: 20,380\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brute-force MC-Distribution-processing neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_aggr_reprb = 8\n",
    "\n",
    "## AGGREGATE REPRESENTATION UNITS\n",
    "## Common network processes distribution-relevant information\n",
    "# x_aggrb = Input(shape = (None,2))\n",
    "# xn_aggrb = BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True,)(x_aggr)\n",
    "# aggr_1b = Dense(4*n_aggr_reprb, activation = 'tanh', kernel_initializer=initializer)(xn_aggr)\n",
    "# aggr_2b = Dense(4*n_aggr_reprb, activation = 'tanh', kernel_initializer=initializer)(aggr_1)\n",
    "# aggr_3b = Dense(2*n_aggr_reprb, activation = 'tanh', kernel_initializer=initializer)(aggr_2)\n",
    "# aggr_4b = Dense(n_aggr_reprb, activation = 'tanh', kernel_initializer=initializer)(aggr_3)\n",
    "\n",
    "## POLICY UNITS\n",
    "# Agent-specific policy units\n",
    "# x_idiob = Input(shape = (None,2*N))\n",
    "# interp_c_h_1b = Dense(32, input_dim=2*N+n_aggr_reprb, activation = 'tanh', kernel_initializer=initializer)(tf.concat(values=(x_idiob,aggr_4b),axis=1))\n",
    "# interp_c_h_2b = Dense(32, activation = 'tanh', kernel_initializer=initializer)(interp_c_h_1b)\n",
    "# policyb = Dense(2*N, activation = 'tanh', kernel_initializer=initializer)(interp_c_h_2b)\n",
    "\n",
    "# nn2 = DENModel(inputs = tf.concat(values=(x_aggrb,x_idiob)), outputs= policyb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = model.initialize_k()\n",
    "eps = draw_eps(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "yp = model.forward(z_h, k, eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "RES = model.residuals(z_h, k, eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUDGET_RES, CSK_RES, CSC_RES, MC_RES = RES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=1032903.3>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.math.reduce_mean(BUDGET_RES*BUDGET_RES + CSK_RES*CSK_RES + CSC_RES*CSC_RES) + MC_RES*MC_RES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "ERR, z, k, eps = model.train_step([z_h,k,eps], batch_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=1117607.4>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ERR"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/1000 [00:01<19:25,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at epoch 0: 1089929.2500\n",
      "Total # time iterations: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 11/1000 [00:11<15:34,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at epoch 10: 42896.0000\n",
      "Total # time iterations: 176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 21/1000 [00:20<16:28,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at epoch 20: 37447.3281\n",
      "Total # time iterations: 336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 31/1000 [00:29<14:31,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at epoch 30: 32227.4961\n",
      "Total # time iterations: 496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 41/1000 [00:39<15:32,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at epoch 40: 27546.9707\n",
      "Total # time iterations: 656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 51/1000 [00:48<14:30,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at epoch 50: 23409.6289\n",
      "Total # time iterations: 816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 61/1000 [00:58<14:53,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at epoch 60: 19754.5059\n",
      "Total # time iterations: 976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 71/1000 [01:08<15:50,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at epoch 70: 16507.7969\n",
      "Total # time iterations: 1136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 81/1000 [01:18<13:54,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at epoch 80: 13612.5029\n",
      "Total # time iterations: 1296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 91/1000 [01:27<13:13,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at epoch 90: 11014.6240\n",
      "Total # time iterations: 1456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 101/1000 [01:36<14:06,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at epoch 100: 8667.4814\n",
      "Total # time iterations: 1616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 111/1000 [01:45<13:44,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at epoch 110: 6535.6870\n",
      "Total # time iterations: 1776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 121/1000 [01:54<13:23,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at epoch 120: 4601.3340\n",
      "Total # time iterations: 1936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 131/1000 [02:03<13:28,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at epoch 130: 2877.6765\n",
      "Total # time iterations: 2096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 141/1000 [02:15<19:00,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at epoch 140: 1403.0759\n",
      "Total # time iterations: 2256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 151/1000 [02:25<13:11,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at epoch 150: 379.7571\n",
      "Total # time iterations: 2416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 161/1000 [02:33<12:30,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at epoch 160: 7.6896\n",
      "Total # time iterations: 2576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 171/1000 [02:42<11:55,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at epoch 170: 32.9961\n",
      "Total # time iterations: 2736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 181/1000 [02:51<11:53,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at epoch 180: 17.9024\n",
      "Total # time iterations: 2896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 191/1000 [03:00<12:24,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at epoch 190: 0.2012\n",
      "Total # time iterations: 3056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 201/1000 [03:10<12:37,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at epoch 200: 1.4242\n",
      "Total # time iterations: 3216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 211/1000 [03:20<12:26,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at epoch 210: 0.7955\n",
      "Total # time iterations: 3376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 221/1000 [03:29<11:44,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at epoch 220: 0.0210\n",
      "Total # time iterations: 3536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 231/1000 [03:38<11:52,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at epoch 230: 0.0657\n",
      "Total # time iterations: 3696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 241/1000 [03:46<11:02,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at epoch 240: 0.0385\n",
      "Total # time iterations: 3856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 251/1000 [03:56<11:41,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at epoch 250: 0.0067\n",
      "Total # time iterations: 4016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 261/1000 [04:04<10:40,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at epoch 260: 0.0093\n",
      "Total # time iterations: 4176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 271/1000 [04:13<10:56,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at epoch 270: 0.0079\n",
      "Total # time iterations: 4336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 281/1000 [04:23<11:31,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at epoch 280: 0.0066\n",
      "Total # time iterations: 4496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 291/1000 [04:31<10:31,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at epoch 290: 0.0066\n",
      "Total # time iterations: 4656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 301/1000 [04:40<10:12,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at epoch 300: 0.0066\n",
      "Total # time iterations: 4816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 311/1000 [04:49<11:00,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at epoch 310: 0.0065\n",
      "Total # time iterations: 4976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 321/1000 [04:58<10:22,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at epoch 320: 0.0065\n",
      "Total # time iterations: 5136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 331/1000 [05:07<10:02,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at epoch 330: 0.0065\n",
      "Total # time iterations: 5296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 341/1000 [05:16<09:42,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at epoch 340: 0.0065\n",
      "Total # time iterations: 5456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 351/1000 [05:25<09:38,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at epoch 350: 0.0065\n",
      "Total # time iterations: 5616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 361/1000 [05:34<09:36,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at epoch 360: 0.0065\n",
      "Total # time iterations: 5776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 371/1000 [05:43<09:14,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at epoch 370: 0.0065\n",
      "Total # time iterations: 5936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 381/1000 [05:52<08:58,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at epoch 380: 0.0065\n",
      "Total # time iterations: 6096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 391/1000 [06:01<09:25,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at epoch 390: 0.0065\n",
      "Total # time iterations: 6256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 401/1000 [06:10<09:04,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at epoch 400: 0.0065\n",
      "Total # time iterations: 6416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 411/1000 [06:20<08:56,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at epoch 410: 0.0065\n",
      "Total # time iterations: 6576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 421/1000 [06:28<08:35,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at epoch 420: 0.0065\n",
      "Total # time iterations: 6736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 431/1000 [06:37<08:31,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at epoch 430: 0.0065\n",
      "Total # time iterations: 6896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 441/1000 [06:46<08:28,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at epoch 440: 0.0065\n",
      "Total # time iterations: 7056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 451/1000 [06:56<08:28,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at epoch 450: 0.0065\n",
      "Total # time iterations: 7216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 461/1000 [07:05<08:15,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at epoch 460: 0.0065\n",
      "Total # time iterations: 7376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 471/1000 [07:14<08:25,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at epoch 470: 0.0065\n",
      "Total # time iterations: 7536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 481/1000 [07:23<07:43,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at epoch 480: 0.0065\n",
      "Total # time iterations: 7696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 491/1000 [07:32<07:48,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at epoch 490: 0.0065\n",
      "Total # time iterations: 7856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 501/1000 [07:43<08:56,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at epoch 500: 0.0065\n",
      "Total # time iterations: 8016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 511/1000 [07:53<07:35,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at epoch 510: 0.0065\n",
      "Total # time iterations: 8176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 521/1000 [08:03<07:18,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at epoch 520: 0.0065\n",
      "Total # time iterations: 8336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 531/1000 [08:12<06:57,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at epoch 530: 0.0065\n",
      "Total # time iterations: 8496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 541/1000 [08:22<07:59,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at epoch 540: 0.0065\n",
      "Total # time iterations: 8656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 551/1000 [08:32<07:04,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at epoch 550: 0.0065\n",
      "Total # time iterations: 8816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 561/1000 [08:41<06:30,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at epoch 560: 0.0065\n",
      "Total # time iterations: 8976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 571/1000 [08:50<06:38,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at epoch 570: 0.0065\n",
      "Total # time iterations: 9136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 581/1000 [08:59<06:19,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at epoch 580: 0.0065\n",
      "Total # time iterations: 9296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 591/1000 [09:09<06:39,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at epoch 590: 0.0065\n",
      "Total # time iterations: 9456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 601/1000 [09:19<07:22,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at epoch 600: 0.0065\n",
      "Total # time iterations: 9616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 611/1000 [09:29<05:43,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at epoch 610: 0.0065\n",
      "Total # time iterations: 9776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 621/1000 [09:42<08:27,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at epoch 620: 0.0065\n",
      "Total # time iterations: 9936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 631/1000 [09:53<06:17,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at epoch 630: 0.0065\n",
      "Total # time iterations: 10096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 641/1000 [10:02<05:19,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at epoch 640: 0.0065\n",
      "Total # time iterations: 10256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 651/1000 [10:11<05:07,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at epoch 650: 0.0065\n",
      "Total # time iterations: 10416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 661/1000 [10:21<06:07,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at epoch 660: 0.0065\n",
      "Total # time iterations: 10576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 671/1000 [10:30<05:08,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at epoch 670: 0.0065\n",
      "Total # time iterations: 10736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 681/1000 [10:40<05:33,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at epoch 680: 0.0065\n",
      "Total # time iterations: 10896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 691/1000 [10:50<04:57,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at epoch 690: 0.0065\n",
      "Total # time iterations: 11056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 701/1000 [11:01<05:22,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at epoch 700: 0.0065\n",
      "Total # time iterations: 11216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 711/1000 [11:11<05:03,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at epoch 710: 0.0065\n",
      "Total # time iterations: 11376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 721/1000 [11:21<04:58,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at epoch 720: 0.0065\n",
      "Total # time iterations: 11536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 731/1000 [11:32<05:24,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at epoch 730: 0.0065\n",
      "Total # time iterations: 11696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 741/1000 [11:42<04:38,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at epoch 740: 0.0065\n",
      "Total # time iterations: 11856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 751/1000 [11:53<03:48,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at epoch 750: 0.0065\n",
      "Total # time iterations: 12016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 761/1000 [12:02<03:52,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at epoch 760: 0.0065\n",
      "Total # time iterations: 12176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 771/1000 [12:12<03:47,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at epoch 770: 0.0065\n",
      "Total # time iterations: 12336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 781/1000 [12:22<03:38,  1.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at epoch 780: 0.0065\n",
      "Total # time iterations: 12496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 791/1000 [12:32<03:23,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at epoch 790: 0.0065\n",
      "Total # time iterations: 12656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 801/1000 [12:42<03:23,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at epoch 800: 0.0065\n",
      "Total # time iterations: 12816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 811/1000 [12:52<03:00,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at epoch 810: 0.0065\n",
      "Total # time iterations: 12976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 821/1000 [13:02<02:48,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at epoch 820: 0.0065\n",
      "Total # time iterations: 13136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 831/1000 [13:12<02:47,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at epoch 830: 0.0065\n",
      "Total # time iterations: 13296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 841/1000 [13:21<02:33,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at epoch 840: 0.0065\n",
      "Total # time iterations: 13456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 851/1000 [13:31<02:31,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at epoch 850: 0.0065\n",
      "Total # time iterations: 13616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 861/1000 [13:41<02:18,  1.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at epoch 860: 0.0065\n",
      "Total # time iterations: 13776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 871/1000 [13:51<01:59,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at epoch 870: 0.0065\n",
      "Total # time iterations: 13936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 881/1000 [14:01<01:54,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at epoch 880: 0.0065\n",
      "Total # time iterations: 14096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 891/1000 [14:11<01:47,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at epoch 890: 0.0065\n",
      "Total # time iterations: 14256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 901/1000 [14:20<01:33,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at epoch 900: 0.0065\n",
      "Total # time iterations: 14416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 911/1000 [14:29<01:26,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at epoch 910: 0.0065\n",
      "Total # time iterations: 14576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 921/1000 [14:39<01:11,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at epoch 920: 0.0065\n",
      "Total # time iterations: 14736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 931/1000 [14:49<01:07,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at epoch 930: 0.0065\n",
      "Total # time iterations: 14896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 941/1000 [14:58<00:57,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at epoch 940: 0.0065\n",
      "Total # time iterations: 15056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 951/1000 [15:08<00:48,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at epoch 950: 0.0065\n",
      "Total # time iterations: 15216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 961/1000 [15:18<00:37,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at epoch 960: 0.0065\n",
      "Total # time iterations: 15376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 971/1000 [15:27<00:28,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at epoch 970: 0.0065\n",
      "Total # time iterations: 15536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 981/1000 [15:37<00:16,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at epoch 980: 0.0065\n",
      "Total # time iterations: 15696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 991/1000 [15:46<00:08,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at epoch 990: 0.0065\n",
      "Total # time iterations: 15856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [15:54<00:00,  1.05it/s]\n"
     ]
    }
   ],
   "source": [
    "metrics = model.train(optimizer=optimizer, n_epochs=1000, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#fig, ax = plt.subplots(1,1)\n",
    "plt.plot(\n",
    "    range(1000),\n",
    "    metrics['mse'],\n",
    ")\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Dynamic Programming & DL",
   "language": "python",
   "name": "dynprog_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
